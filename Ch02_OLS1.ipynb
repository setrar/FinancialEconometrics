{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic OLS\n",
    "\n",
    "This notebook estimates a linear regression and reports standard errors (assuming iid residuals).\n",
    "\n",
    "For a package, consider [GLM.jl](https://github.com/JuliaStats/GLM.jl) (not used here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages and Extra Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "printyellow (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Printf, DelimitedFiles, Statistics, LinearAlgebra, Distributions\n",
    "\n",
    "include(\"jlFiles/printmat.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size:    (388,)\n"
     ]
    }
   ],
   "source": [
    "x = readdlm(\"Data/FFmFactorsPs.csv\",',',skipstart=1)\n",
    "\n",
    "                #yearmonth, market, small minus big, high minus low\n",
    "(ym,Rme,RSMB,RHML) = (x[:,1],x[:,2]/100,x[:,3]/100,x[:,4]/100) \n",
    "x = nothing\n",
    "\n",
    "printlnPs(\"Sample size:\",size(Rme))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS Estimates and Their Distribution\n",
    "\n",
    "Consider the linear regression\n",
    "\n",
    "$\n",
    "y_{t}=\\beta^{\\prime}x_{t}+u_{t},\n",
    "$\n",
    "\n",
    "where $y_{t}$ is a scalar and $x_{t}$ is $k\\times1$. The OLS estimate is\n",
    "\n",
    "$\n",
    "\\hat{\\beta} = S_{xx}^{-1}S_{xy}, \\: \\text{ where } \\: \n",
    "S_{xx}      = \\sum\\nolimits_{t=1}^{T}x_{t}x_{t}^{\\prime}\n",
    "\\: \\text{ and } \\:\n",
    "S_{xy}      = \\sum\\nolimits_{t=1}^{T}x_{t}y_{t}.\n",
    "$\n",
    "\n",
    "When $x_t$ and $u_t$ are independent and $u_t$ is iid (Gauss-Markov assumptions), then the distribution of the estimates is (typically)\n",
    "\n",
    "$\n",
    "(\\hat{\\beta}-\\beta_{0})\\overset{d}{\\rightarrow}N(0,S_{xx}^{-1}\\sigma^2),\n",
    "$\n",
    "\n",
    "where $\\sigma^2$ is the variance of the residual.\n",
    "\n",
    "\n",
    "### A Remark on the Code\n",
    "\n",
    "To calculate the estimates it is often convenient to work with matrices. Define \n",
    "\n",
    "$X_{T\\times k}$ and $Y_{T\\times1}$\n",
    "by letting $x_{t}^{\\prime}$ and $y_{t}$ be the $t^{th}$ rows\n",
    "\n",
    "$$\n",
    "X_{T\\times k}=\n",
    "\\begin{bmatrix}\n",
    "x_{1}^{\\prime}\\\\\n",
    "\\vdots\\\\\n",
    "x_{T}^{\\prime}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y_{T\\times1}=\n",
    "\\begin{bmatrix}\n",
    "y_{1}\\\\\n",
    "\\vdots\\\\\n",
    "y_{T}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "The estimates can then be calculated as \n",
    "```\n",
    "b = X\\Y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Function for OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OlsGMFn"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    OlsGMFn(Y,X)\n",
    "\n",
    "LS of Y on X; for one dependent variable, Gauss-Markov assumptions\n",
    "\n",
    "# Usage\n",
    "(b,u,ŷ,V,R²) = OlsGMFn(Y,X)\n",
    "\n",
    "# Input\n",
    "- `Y::Vector`:    T-vector, the dependent variable\n",
    "- `X::Matrix`:    Txk matrix of regressors (including deterministic ones)\n",
    "\n",
    "# Output\n",
    "- `b::Vector`:    k-vector, regression coefficients\n",
    "- `u::Vector`:    T-vector, residuals Y - ŷ\n",
    "- `ŷ::Vector`:    T-vector, fitted values X*b\n",
    "- `V::Matrix`:    kxk matrix, covariance matrix of b\n",
    "- `R²::Number`:   scalar, R² value\n",
    "\n",
    "\"\"\"\n",
    "function OlsGMFn(Y,X)\n",
    "\n",
    "    T    = size(Y,1)\n",
    "\n",
    "    b    = X\\Y\n",
    "    ŷ    = X*b                #y\\hat[TAB]\n",
    "    u    = Y - ŷ\n",
    "\n",
    "    σ²   = var(u)             #\\sigma\\^2[TAB]\n",
    "    V    = inv(X'X)*σ²\n",
    "    R²   = 1 - σ²/var(Y)\n",
    "\n",
    "    return b, u, ŷ, V, R²\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mOLS Results:\u001b[22m\u001b[39m\n",
      "\n",
      "            b       std\n",
      "c       0.007     0.002\n",
      "SMB     0.217     0.073\n",
      "HML    -0.429     0.074\n",
      "\n",
      "      R²:      0.134\n",
      "       b:      0.007     0.217    -0.429\n",
      "       X:      1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     1.000     0.037     0.005     0.032     0.022     0.003     0.012     0.013     0.021    -0.003    -0.033     0.028     0.042     0.017    -0.018    -0.066     0.010     0.021     0.017     0.042     0.039     0.009     0.025    -0.035    -0.002     0.030    -0.003     0.036     0.044     0.020    -0.009    -0.023    -0.019    -0.026     0.022    -0.009     0.012    -0.012     0.005    -0.002     0.015     0.005    -0.004     0.009    -0.041     0.029     0.023     0.048    -0.002     0.027     0.033     0.018     0.005     0.061     0.009     0.015    -0.043     0.006    -0.036     0.020    -0.003    -0.004    -0.017     0.001    -0.012     0.001    -0.003    -0.023    -0.002     0.002    -0.012    -0.006    -0.006     0.033     0.008    -0.011     0.002    -0.023     0.005     0.029    -0.004    -0.016    -0.016     0.002    -0.005     0.012    -0.006    -0.005     0.028    -0.013    -0.009    -0.034    -0.042     0.023    -0.026    -0.020     0.001    -0.017     0.035     0.004    -0.017    -0.006    -0.021    -0.007    -0.008     0.006    -0.084     0.026     0.002    -0.007     0.033     0.062     0.010    -0.026     0.021    -0.002     0.001    -0.013    -0.029    -0.017     0.019    -0.021     0.027     0.007    -0.005    -0.000    -0.010    -0.040     0.005     0.003    -0.032    -0.013    -0.024    -0.013     0.011     0.015    -0.004    -0.025     0.014    -0.032    -0.036    -0.036    -0.056     0.003     0.008     0.038     0.039     0.039     0.005    -0.003     0.001    -0.010     0.016     0.017     0.009    -0.006    -0.023     0.084     0.009    -0.011    -0.061     0.004    -0.031    -0.004    -0.001     0.006     0.021     0.036     0.016     0.020    -0.034     0.002    -0.006     0.020    -0.003     0.009     0.002     0.031     0.014    -0.014     0.012     0.001     0.027    -0.009    -0.009    -0.020    -0.005    -0.018     0.014     0.027    -0.022    -0.002     0.000    -0.030    -0.003    -0.003    -0.004    -0.022     0.031     0.022     0.018    -0.020    -0.040    -0.008     0.004    -0.024     0.021     0.013     0.049     0.032    -0.037    -0.036     0.023    -0.008    -0.041    -0.036     0.032    -0.015    -0.026    -0.003    -0.051     0.048     0.015    -0.024     0.075     0.027    -0.008    -0.050    -0.023    -0.010     0.002    -0.012     0.005    -0.036    -0.033    -0.049    -0.057    -0.002    -0.032     0.011    -0.003     0.007    -0.057    -0.039     0.034     0.037     0.034     0.022    -0.012     0.033    -0.068     0.077     0.068     0.044     0.221    -0.166    -0.076    -0.047     0.137    -0.028    -0.009    -0.018    -0.036    -0.031     0.016     0.070    -0.011     0.005     0.003     0.030     0.064    -0.042     0.022    -0.065     0.068     0.004     0.051     0.011    -0.017     0.043     0.058    -0.037     0.035    -0.052    -0.022     0.027    -0.030     0.032    -0.005     0.014    -0.003     0.008     0.011     0.047     0.015     0.056     0.026     0.006     0.029     0.022    -0.028     0.026    -0.011     0.019    -0.026    -0.002     0.023    -0.038    -0.015     0.029     0.004     0.041     0.002    -0.016    -0.007    -0.013    -0.040     0.030     0.026     0.028    -0.009    -0.006    -0.010     0.010    -0.005     0.054    -0.004     0.035    -0.012    -0.030    -0.005    -0.039     0.008    -0.012     0.017     0.007    -0.009     0.001     0.014    -0.002    -0.021    -0.001     0.007    -0.027    -0.001    -0.025     0.001    -0.027     0.001    -0.008    -0.006     0.009    -0.015     0.028     0.011     0.037     0.038    -0.002    -0.022    -0.036     0.040    -0.009    -0.004     0.007     0.052    -0.026     0.026     0.025    -0.006     0.023    -0.043    -0.028     0.059     0.004     0.014     0.016     0.050    -0.000    -0.021     0.001    -0.029     0.040     0.009     0.038     0.008    -0.025     0.018     0.027    -0.002     0.023     0.012    -0.007     0.011     0.016     0.014     0.018    -0.015    -0.009    -0.019    -0.033    -0.020     0.018     0.006    -0.010     0.011     0.004    -0.009    -0.063    -0.027    -0.047    -0.028    -0.085     0.027     0.069     0.010     0.007     0.023    -0.005     0.051    -0.005     0.048     0.052    -0.044     0.019     0.007     0.031     0.060     0.039    -0.027     0.018     0.016     0.002     0.012     0.003    -0.037    -0.019     0.000    -0.009     0.007     0.021     0.006    -0.014    -0.038     0.057     0.055     0.011     0.049    -0.006     0.016     0.076     0.034     0.005     0.013     0.002    -0.026     0.003    -0.018     0.053     0.005     0.040    -0.001    -0.053    -0.002     0.040     0.037    -0.009     0.004    -0.016     0.022     0.013     0.007    -0.029    -0.015     0.005    -0.008    -0.004    -0.029    -0.001     0.014     0.047     0.035     0.032    -0.015    -0.002     0.004    -0.032    -0.060     0.017    -0.003     0.002     0.011     0.007    -0.009     0.003     0.042     0.032    -0.045     0.052    -0.017     0.008     0.017     0.023    -0.011     0.023     0.021    -0.007     0.017     0.013    -0.015     0.005     0.009     0.005    -0.014    -0.008     0.022    -0.028     0.007    -0.013    -0.010    -0.011     0.003     0.009     0.007    -0.029    -0.025    -0.036    -0.021    -0.001     0.015     0.007     0.003    -0.030    -0.015    -0.017    -0.006    -0.012     0.014    -0.006     0.012    -0.013    -0.008    -0.011    -0.004    -0.018    -0.040     0.046     0.064     0.037     0.043     0.012     0.033    -0.005    -0.011    -0.002    -0.020    -0.015     0.025     0.058     0.065     0.012     0.026    -0.035     0.026     0.032    -0.004    -0.005    -0.016    -0.003     0.006     0.021    -0.014     0.013     0.017     0.001     0.017     0.010    -0.035    -0.018    -0.024    -0.001     0.002     0.017     0.004    -0.020     0.017     0.020    -0.030    -0.021     0.019    -0.009    -0.001     0.003     0.014     0.004    -0.023     0.012    -0.040    -0.014     0.019     0.044    -0.006    -0.038     0.048     0.002     0.009    -0.023     0.047     0.039    -0.010    -0.043     0.007    -0.004     0.010    -0.003     0.023     0.010     0.038    -0.018    -0.007     0.017     0.003     0.043    -0.018    -0.011     0.051    -0.040    -0.027    -0.033    -0.048    -0.056     0.017    -0.029     0.025     0.027    -0.043     0.005    -0.010    -0.031    -0.033    -0.080    -0.092     0.002    -0.129     0.079     0.093     0.038    -0.099     0.084    -0.013     0.069     0.048     0.124     0.061    -0.057     0.139     0.064    -0.044     0.028    -0.021     0.056     0.033     0.016    -0.070     0.008     0.004     0.035     0.039     0.011     0.042     0.025     0.015    -0.036     0.022     0.012    -0.065    -0.015     0.039    -0.009    -0.015    -0.017    -0.000     0.002     0.007    -0.021     0.018     0.009     0.018     0.015     0.026     0.017     0.003    -0.000    -0.017    -0.003     0.017     0.044     0.011     0.003    -0.009     0.019    -0.003     0.025     0.028     0.017    -0.005    -0.012     0.028    -0.005     0.014     0.012    -0.007    -0.018     0.005     0.011    -0.008    -0.001     0.031     0.028     0.015     0.033    -0.017    -0.004     0.005     0.005     0.025    -0.001     0.002     0.004    -0.010    -0.001    -0.010    -0.029    -0.024    -0.021    -0.019    -0.010    -0.001     0.030     0.001     0.002     0.001    -0.003    -0.011     0.036     0.015     0.044    -0.031    -0.050    -0.012    -0.099    -0.068     0.026     0.057     0.005    -0.025     0.048     0.077     0.015    -0.044     0.001     0.007     0.006     0.028     0.020     0.031    -0.024    -0.043     0.002    -0.017    -0.030    -0.022    -0.007     0.035     0.010     0.014    -0.013    -0.023\n",
      "       Y:      0.042    -0.034     0.058     0.001    -0.022     0.039     0.007     0.057    -0.007    -0.081     0.054     0.019     0.058    -0.008    -0.132     0.040     0.052     0.032     0.064     0.017     0.022     0.011     0.095    -0.048    -0.050     0.005     0.034    -0.022     0.002    -0.024    -0.015    -0.069    -0.076     0.048     0.035    -0.037    -0.034    -0.060    -0.020     0.032    -0.039    -0.034    -0.031     0.111     0.012     0.113     0.046     0.008     0.035     0.024     0.028     0.067     0.006     0.031    -0.039    -0.004     0.009    -0.036     0.023    -0.018    -0.021    -0.046     0.006    -0.006    -0.060     0.016    -0.029     0.104    -0.008    -0.010    -0.018     0.017     0.079     0.011    -0.008    -0.009     0.049     0.012    -0.007    -0.010    -0.046     0.038     0.063     0.037     0.004     0.067     0.048    -0.013     0.046     0.009    -0.065     0.062    -0.083     0.045     0.011    -0.031     0.124     0.044     0.019    -0.021     0.001     0.039     0.040     0.032    -0.025    -0.231    -0.076     0.066     0.042     0.047    -0.021     0.006    -0.005     0.047    -0.012    -0.034     0.031     0.011    -0.022     0.015     0.061    -0.022     0.015     0.042     0.031    -0.012     0.070     0.015    -0.008    -0.036     0.011     0.012    -0.076     0.009     0.018    -0.035     0.082    -0.011    -0.016    -0.098    -0.060    -0.019     0.060     0.024     0.044     0.071     0.025    -0.002     0.036    -0.048     0.042     0.022    -0.016     0.014    -0.041     0.103    -0.005     0.011    -0.027     0.010     0.004    -0.022     0.037    -0.023     0.010     0.009     0.038     0.015     0.010     0.003     0.023    -0.028     0.027     0.003    -0.003     0.037    -0.002     0.016    -0.020     0.017     0.029    -0.026    -0.048     0.007     0.006    -0.031     0.028     0.039    -0.022     0.011    -0.041     0.008     0.016     0.036     0.022     0.021     0.029     0.026     0.036     0.005     0.032    -0.016     0.038     0.010     0.024     0.012     0.007     0.021     0.023    -0.012    -0.058     0.028     0.049     0.009     0.062    -0.016     0.049    -0.005    -0.049     0.038     0.067     0.040     0.072    -0.040     0.054    -0.039     0.026     0.013     0.000     0.069     0.047     0.007    -0.030     0.028    -0.027    -0.162     0.059     0.071     0.059     0.059     0.035    -0.042     0.034     0.045    -0.024     0.047    -0.035    -0.014    -0.027     0.058     0.033     0.079    -0.044     0.028     0.049    -0.064    -0.044     0.048    -0.022     0.071    -0.056    -0.030    -0.108     0.015     0.034    -0.103    -0.075     0.080     0.007    -0.020    -0.021    -0.062    -0.094     0.026     0.077     0.016    -0.017    -0.023     0.043    -0.051    -0.012    -0.072    -0.083     0.007    -0.101     0.073     0.060    -0.054    -0.024    -0.016     0.009     0.082     0.063     0.015     0.022     0.024    -0.010     0.060     0.016     0.045     0.022     0.015    -0.012    -0.025     0.014     0.021    -0.039     0.002     0.019     0.017     0.047     0.034    -0.028     0.021    -0.019    -0.027     0.036     0.009     0.041    -0.009     0.008    -0.024     0.037     0.000     0.036    -0.005     0.015     0.009    -0.035    -0.004    -0.006     0.021     0.015     0.033     0.019     0.007     0.015    -0.018     0.009     0.035     0.035    -0.019    -0.036     0.007     0.038     0.023    -0.053    -0.007    -0.064    -0.023    -0.012     0.049     0.022    -0.080    -0.015     0.010    -0.100    -0.185    -0.086     0.021    -0.077    -0.101     0.087     0.110     0.067    -0.003     0.082     0.032     0.045    -0.028     0.057     0.029    -0.037     0.035     0.064     0.020    -0.080    -0.052     0.071    -0.044     0.092     0.039     0.006     0.068     0.020     0.038     0.003     0.028\n"
     ]
    }
   ],
   "source": [
    "Y = Rme                    #to get standard OLS notation\n",
    "T = size(Y,1)\n",
    "X = [ones(T) RSMB RHML]\n",
    "\n",
    "(b,_,_,V,R²) = OlsGMFn(Y,X)\n",
    "Stdb = sqrt.(diag(V))        #standard error\n",
    "\n",
    "printblue(\"OLS Results:\\n\")\n",
    "xNames = [\"c\",\"SMB\",\"HML\"]\n",
    "printmat([b Stdb],colNames=[\"b\",\"std\"],rowNames=xNames)\n",
    "\n",
    "printlnPs(\"R²: \",R²)\n",
    "printlnPs(\"b: \",b)\n",
    "printlnPs(\"X: \",X)\n",
    "printlnPs(\"Y: \",Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values (extra)\n",
    "\n",
    "The next cells use some simple functions to remove observations ($t$) where $y_t$ and/or some of the $x_t$ variables are NaN/missing. We illustrate the usage by a very simple example.\n",
    "\n",
    "An alternative approach is to fill *both* $y_t$ and $x_t$ with zeros (if any of them contains NaN/missing) and then do the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"jlFiles/excise.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "         y        x1        x2\n",
      "     1.000     1.000    11.000\n",
      "     2.000     1.000       NaN\n",
      "     3.000     1.000    13.500\n",
      "\n",
      "after\n",
      "         y        x1        x2\n",
      "     1.000     1.000    11.000\n",
      "     3.000     1.000    13.500\n",
      "\n",
      "OLS using only observations without any NaN/missing\n",
      "    -7.800\n",
      "     0.800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = [1,2,3.0]\n",
    "x = [1 11;\n",
    "     1 NaN;\n",
    "     1 13.5]\n",
    "\n",
    "(y1,x1) = excise(y,x)\n",
    "println(\"before\")\n",
    "printmat(y,x,colNames=[\"y\",\"x1\",\"x2\"])\n",
    "\n",
    "println(\"after\")\n",
    "printmat(y1,x1,colNames=[\"y\",\"x1\",\"x2\"])\n",
    "\n",
    "println(\"OLS using only observations without any NaN/missing\")\n",
    "b = x1\\y1\n",
    "printmat(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS from setting observations with any NaN/missing to 0\n",
      "    -7.800\n",
      "     0.800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vv = FindNNPs(y,x)             #vv[t] = true if no missing/NaN i (y[t],x[t,:])\n",
    "\n",
    "(y2,x2)     = (copy(y),copy(x))    #set to 0 if any missing/NaN\n",
    "y2[.!vv  ] .=  0\n",
    "x2[.!vv,:] .= 0   \n",
    "\n",
    "println(\"OLS from setting observations with any NaN/missing to 0\")\n",
    "b = x2\\y2\n",
    "printmat(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Ways to Calculate OLS Estimates (extra)\n",
    "\n",
    "The next cell calculates the OLS estimates in three different ways: (1) a loop to create $S_{xx}$ and $S_{xy}$ followed by $S_{xx}^{-1}S_{xy}$; (2) $(X'X)^{-1}X'Y$; (3) and `X\\Y`. They should give the same result in well-behaved data sets, but (3) is probably the most stable version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mThree different ways to calculate OLS estimates:\u001b[22m\u001b[39m\n",
      "           b1        b2        b3\n",
      "c       0.007     0.007     0.007\n",
      "SMB     0.217     0.217     0.217\n",
      "HML    -0.429    -0.429    -0.429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printblue(\"Three different ways to calculate OLS estimates:\")\n",
    "\n",
    "k    = size(X,2)\n",
    "Sxx = zeros(k,k)\n",
    "Sxy = zeros(k,1)\n",
    "for t = 1:T\n",
    "    #local x_t, y_t            #local/global is needed in script\n",
    "    #global Sxx, Sxy\n",
    "    x_t = X[t,:]               #a vector\n",
    "    y_t = Y[t]\n",
    "    Sxx = Sxx + x_t*x_t'     #kxk, same as Sxx += x_t*x_t'\n",
    "    Sxy = Sxy + x_t*y_t      #kx1, same as Sxy += x_t*y_t\n",
    "end\n",
    "b1 = inv(Sxx)*Sxy            #OLS coeffs, version 1\n",
    "\n",
    "b2 = inv(X'X)*X'Y            #OLS coeffs, version 2\n",
    "\n",
    "b3 = X\\Y                     #OLS coeffs, version 3\n",
    "\n",
    "printmat([b1 b2 b3],colNames=[\"b1\",\"b2\",\"b3\"],rowNames=xNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
